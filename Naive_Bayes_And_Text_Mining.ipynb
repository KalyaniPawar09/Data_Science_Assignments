{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data package\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/blogs (1).csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Remove metadata and unnecessary headers\n",
        "    text = re.sub(r'(?i)(path:|newsgroups:|xref:).*?\\n', '', text)\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    text = re.sub(r'[\\W_]+', ' ', text).lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['Cleaned_Data'] = data['Data'].apply(preprocess_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLwn5Upawjog",
        "outputId": "3335f2b2-5745-4cfc-d191-68bf5c4c52f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(data['Cleaned_Data'])\n",
        "y = data['Labels']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Sentiment Analysis\n",
        "def analyze_sentiment(text):\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply sentiment analysis\n",
        "data['Sentiment'] = data['Cleaned_Data'].apply(analyze_sentiment)\n",
        "\n",
        "# Sentiment distribution by category\n",
        "sentiment_distribution = data.groupby(['Labels', 'Sentiment']).size().unstack(fill_value=0)\n",
        "print(\"\\nSentiment Distribution by Category:\\n\", sentiment_distribution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lvu3_Nuv6Dm",
        "outputId": "4f4bba9e-040c-4f22-9917-a86ea307c700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7225\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.56      0.83      0.67        18\n",
            "           comp.graphics       0.65      0.72      0.68        18\n",
            " comp.os.ms-windows.misc       0.76      0.86      0.81        22\n",
            "comp.sys.ibm.pc.hardware       0.64      0.56      0.60        25\n",
            "   comp.sys.mac.hardware       0.73      0.52      0.61        21\n",
            "          comp.windows.x       0.83      0.40      0.54        25\n",
            "            misc.forsale       0.65      0.61      0.63        18\n",
            "               rec.autos       0.67      0.89      0.76        18\n",
            "         rec.motorcycles       0.75      0.75      0.75        16\n",
            "      rec.sport.baseball       0.65      0.83      0.73        18\n",
            "        rec.sport.hockey       0.58      1.00      0.73        15\n",
            "               sci.crypt       0.72      0.95      0.82        19\n",
            "         sci.electronics       0.50      0.50      0.50        16\n",
            "                 sci.med       0.82      0.82      0.82        17\n",
            "               sci.space       1.00      0.81      0.89        21\n",
            "  soc.religion.christian       0.85      1.00      0.92        23\n",
            "      talk.politics.guns       0.81      0.75      0.78        28\n",
            "   talk.politics.mideast       0.95      0.95      0.95        20\n",
            "      talk.politics.misc       0.73      0.61      0.67        18\n",
            "      talk.religion.misc       0.70      0.29      0.41        24\n",
            "\n",
            "                accuracy                           0.72       400\n",
            "               macro avg       0.73      0.73      0.71       400\n",
            "            weighted avg       0.74      0.72      0.71       400\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  2]\n",
            " [ 0 13  0  0  0  0  1  0  0  1  1  1  1  0  0  0  0  0  0  0]\n",
            " [ 0  0 19  1  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
            " [ 0  1  2 14  0  0  2  1  1  1  0  1  2  0  0  0  0  0  0  0]\n",
            " [ 0  2  0  2 11  1  1  1  0  0  1  1  1  0  0  0  0  0  0  0]\n",
            " [ 0  2  2  2  1 10  0  0  2  1  1  2  1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  2  0 11  2  0  2  0  0  1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1 16  0  0  0  0  1  0  0  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  0  1 12  0  1  0  0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 15  3  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  1 18  0  0  0  0  0  0  0  0]\n",
            " [ 0  1  1  1  1  0  0  1  0  1  1  0  8  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  1  0  0  0  0  1 14  0  0  0  0  0  0]\n",
            " [ 0  1  0  0  0  0  1  0  0  1  1  0  0  0 17  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  0  0  0  0]\n",
            " [ 1  0  0  1  0  0  0  0  1  0  0  0  0  1  0  0 21  0  3  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0 19  0  0]\n",
            " [ 0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  1  2  1 11  1]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  2  0  1  7]]\n",
            "\n",
            "Sentiment Distribution by Category:\n",
            " Sentiment                 Negative  Neutral  Positive\n",
            "Labels                                               \n",
            "alt.atheism                     32        0        68\n",
            "comp.graphics                   27        0        73\n",
            "comp.os.ms-windows.misc         20        1        79\n",
            "comp.sys.ibm.pc.hardware        18        0        82\n",
            "comp.sys.mac.hardware           26        0        74\n",
            "comp.windows.x                  19        2        79\n",
            "misc.forsale                    20        0        80\n",
            "rec.autos                       20        0        80\n",
            "rec.motorcycles                 28        0        72\n",
            "rec.sport.baseball              35        0        65\n",
            "rec.sport.hockey                38        0        62\n",
            "sci.crypt                       20        0        80\n",
            "sci.electronics                 23        0        77\n",
            "sci.med                         34        0        66\n",
            "sci.space                       27        0        73\n",
            "soc.religion.christian          23        0        77\n",
            "talk.politics.guns              39        1        60\n",
            "talk.politics.mideast           28        0        72\n",
            "talk.politics.misc              25        0        75\n",
            "talk.religion.misc              20        0        80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uX_0sWuTv6Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pphyW7Usv59u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QQAYSKv6v56_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}